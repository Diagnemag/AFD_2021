{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP_1-ENSAE21-solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Diagnemag/AFD_2021/blob/main/TP_FactorizationMachine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX7NZ1PG2TXt"
      },
      "source": [
        "#Goals of the practical session:\n",
        "- Understand a mimimal Python code implementing a memory efficient logistic regression from scratch. \n",
        "- Adapt the code to obtain some common features such as a quadratic kernel and a regularization.\n",
        "- Learn how to conveniently track and measure the performances of a set of models with several hyper parameters. \n",
        "\n",
        "#Part I: Logistic regression from scratch\n",
        "\n",
        "Download some data from a recent challenge\n",
        "- [Small version](http://go.criteo.net/criteo-ppml-challenge-adkdd21-dataset.zip) (284 Mo, more than enough for today)\n",
        "- [Complete version](http://go.criteo.net/criteo-ppml-challenge-adkdd21-dataset-raw-granular-data.csv.gz) (optional aditionnal data if you want to try different things - 50Go)\n",
        "\n",
        "For convience here it is a few batch commands downloading and extracting data in your drive:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJT1-3d62K22",
        "outputId": "7c2bf282-539e-4ae6-cab3-d3eb8f115aa6"
      },
      "source": [
        "!wget http://go.criteo.net/criteo-ppml-challenge-adkdd21-dataset.zip\n",
        "!unzip criteo-ppml-challenge-adkdd21-dataset.zip\n",
        "!rm aggregated_noisy_data_pairs.csv.gz aggregated_noisy_data_singles.csv.gz  \n",
        "!gunzip X_test.csv.gz X_train.csv.gz y_test.csv.gz y_train.csv.gz\n",
        "!rm criteo-ppml-challenge-adkdd21-dataset.zip\n",
        "!sed -n 1,10000p X_test.csv > X_valid.csv #CARE we took 10k lines from test as validation.\n",
        "!sed -n 1,10000p y_test.csv > y_valid.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-21 15:20:27--  http://go.criteo.net/criteo-ppml-challenge-adkdd21-dataset.zip\n",
            "Resolving go.criteo.net (go.criteo.net)... 178.250.0.152\n",
            "Connecting to go.criteo.net (go.criteo.net)|178.250.0.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://criteostorage.blob.core.windows.net/criteo-research-datasets/criteo_ppml_challenge_adkdd2021/criteo-ppml-challenge-adkdd21-dataset.zip [following]\n",
            "--2021-12-21 15:20:28--  https://criteostorage.blob.core.windows.net/criteo-research-datasets/criteo_ppml_challenge_adkdd2021/criteo-ppml-challenge-adkdd21-dataset.zip\n",
            "Resolving criteostorage.blob.core.windows.net (criteostorage.blob.core.windows.net)... 20.209.1.1\n",
            "Connecting to criteostorage.blob.core.windows.net (criteostorage.blob.core.windows.net)|20.209.1.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 297834690 (284M) [application/zip]\n",
            "Saving to: ‘criteo-ppml-challenge-adkdd21-dataset.zip’\n",
            "\n",
            "criteo-ppml-challen 100%[===================>] 284.04M   139MB/s    in 2.0s    \n",
            "\n",
            "2021-12-21 15:20:30 (139 MB/s) - ‘criteo-ppml-challenge-adkdd21-dataset.zip’ saved [297834690/297834690]\n",
            "\n",
            "Archive:  criteo-ppml-challenge-adkdd21-dataset.zip\n",
            "  inflating: aggregated_noisy_data_pairs.csv.gz  \n",
            "  inflating: aggregated_noisy_data_singles.csv.gz  \n",
            "  inflating: X_test.csv.gz           \n",
            "  inflating: X_train.csv.gz          \n",
            "  inflating: y_test.csv.gz           \n",
            "  inflating: y_train.csv.gz          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjxqG5PTXy94"
      },
      "source": [
        "A quick look to the 10 first rows of the data shows there is 19 categorical features already hashed as integers. All of them are related to the proability of a click event and are related to observed events of a user either on a publisher website, either on an advertiser website. The large dataset correspond to one day of data on one platform. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5vpJ7qb3kzP",
        "outputId": "36f544e7-e10d-48f6-e2ac-9255bc351cb9"
      },
      "source": [
        "!head X_test.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hash_0,hash_1,hash_2,hash_3,hash_4,hash_5,hash_6,hash_7,hash_8,hash_9,hash_10,hash_11,hash_12,hash_13,hash_14,hash_15,hash_16,hash_17,hash_18\n",
            "187652,199446,480789,-84947,-419788,-356697,-73473,-201336,336272,39558,171177,-332795,-197733,-102727,-267888,-324676,172595,-1100,182002\n",
            "-196263,-137304,480789,-428062,347621,-310777,220982,-201336,336272,301505,406906,-265634,467836,-437971,26095,-328942,-400942,-429456,346045\n",
            "-216157,-137304,480789,-428062,347621,-310777,167271,-201336,359042,-311089,306285,-332795,-283182,-102727,495950,-324676,326477,-317260,182002\n",
            "11652,-432181,480789,-169356,287595,-310777,-332084,-201336,310671,409356,-496385,-332795,236957,-102727,-20670,-324676,-153563,258932,-124383\n",
            "224181,-455384,313949,-428062,166559,-310777,167271,-201336,127803,409356,-23318,-332795,286937,35444,335698,-324676,132133,-355723,-124383\n",
            "486387,-432181,480789,-169356,-419788,-356697,-484965,-201336,-428527,-139213,25441,-468602,315199,-102727,-421972,-241726,306269,-271996,346045\n",
            "442096,-411158,480789,-169356,-419788,-356697,-438087,-201336,-22347,-237302,-43085,-449303,-350579,-102727,193941,32790,26518,-27708,346045\n",
            "-47443,-432181,480789,-2679,347621,-310777,167271,-201336,336272,-105474,-7296,-332795,286937,-102727,-103791,-324676,326477,-251246,182002\n",
            "445274,-432181,480789,-428062,287595,-356697,-438087,-201336,-22347,39558,207091,-332795,-473714,-13772,335698,-324676,14846,311823,182002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPt7pg7NOcc3",
        "outputId": "7257bcab-7321-4775-c508-1cf71d12f00a"
      },
      "source": [
        "!head y_test.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "click,sale\n",
            "1.0,0.0\n",
            "0.0,0.0\n",
            "0.0,0.0\n",
            "1.0,0.0\n",
            "0.0,0.0\n",
            "0.0,0.0\n",
            "0.0,0.0\n",
            "0.0,0.0\n",
            "0.0,0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amuBvEE72Tno"
      },
      "source": [
        "Form the small dataset, it is possible to directly use almost any software able to perform a logistic regression. Depending on your hardware you may run in troubles for the large one (in fact with the large one vowpal wabbit will do it, but strugles to achieve the optimization of the loss). \n",
        "\n",
        "Today our first goal is to understand a minimal version of a logistic regression coded from scratch in Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQjRc1Bi5VN9",
        "outputId": "a0a985f1-3d48-41ea-de9e-046c9446fe06"
      },
      "source": [
        "from datetime import datetime\n",
        "from csv import DictReader\n",
        "from math import exp, log, sqrt\n",
        "\n",
        "\n",
        "# parameters #################################################################\n",
        "\n",
        "X_train = 'X_train.csv'  # path to training files\n",
        "y_train = 'y_train.csv'\n",
        "\n",
        "\n",
        "D = 2 ** 20   # max number of weights to use (hash size)\n",
        "alpha = .1    # learning rate for sgd optimization\n",
        "\n",
        "\n",
        "# function definitions #######################################################\n",
        "\n",
        "# A. Bounded logloss\n",
        "# INPUT:\n",
        "#     p: our prediction\n",
        "#     y: real answer\n",
        "# OUTPUT\n",
        "#     logarithmic loss of p given y\n",
        "def logloss(p, y):\n",
        "    p = max(min(p, 1. - 10e-12), 10e-12)\n",
        "    return -log(p) if y == 1. else -log(1. - p)\n",
        "\n",
        "\n",
        "# B. Apply hash trick of the original csv row\n",
        "# for simplicity, we treat both integer and categorical features as categorical\n",
        "# INPUT:\n",
        "#     csv_row: a csv dictionary, ex: {'hash_1': '357', 'hash_2': '', ...}\n",
        "#     D: the max index that we can hash to\n",
        "# OUTPUT:\n",
        "#     x: a list of indices that its value is 1\n",
        "def get_x(csv_row, D):\n",
        "    x = []\n",
        "    for key, value in csv_row.items():\n",
        "        index = int(value + key[4:], 16) % D \n",
        "        x.append(index)\n",
        "    return x  # x contains indices of features that have a value of 1\n",
        "\n",
        "\n",
        "# C. Get probability estimation on x\n",
        "# INPUT:\n",
        "#     x: features\n",
        "#     w: weights\n",
        "# OUTPUT:\n",
        "#     probability of p(y = 1 | x; w)\n",
        "def get_p(x, w):\n",
        "    wTx = 0.\n",
        "    for i in x:  # do wTx\n",
        "        wTx += w[i] * 1.  # w[i] * x[i], but if i in x we got x[i] = 1.\n",
        "    return 1. / (1. + exp(-max(min(wTx, 20.), -20.)))  # bounded sigmoid\n",
        "\n",
        "\n",
        "# D. Update given model\n",
        "# INPUT:\n",
        "#     w: weights\n",
        "#     n: a counter that counts the number of times we encounter a feature\n",
        "#        this is used for adaptive learning rate\n",
        "#     x: feature\n",
        "#     p: prediction of our model\n",
        "#     y: answer\n",
        "# OUTPUT:\n",
        "#     w: updated model\n",
        "#     n: updated count\n",
        "def update_w(w, n, x, p, y):\n",
        "    for i in x:\n",
        "        # alpha / (sqrt(n) + 1) is the adaptive learning rate heuristic\n",
        "        # (p - y) * x[i] is the current gradient\n",
        "        # note that in our case, if i in x then x[i] = 1\n",
        "        w[i] -= (p - y) * alpha / (sqrt(n[i]) + 1.)\n",
        "        n[i] += 1.\n",
        "    return w, n\n",
        "\n",
        "\n",
        "# training and testing #######################################################\n",
        "\n",
        "# initialize our model\n",
        "w = [0.] * D  # first order parameters\n",
        "n = [0.] * D  # number of times we've encountered a feature\n",
        "\n",
        "# start training a logistic regression model using on pass sgd\n",
        "loss = 0.\n",
        "for t, (row, y)  in enumerate(zip(DictReader(open(X_train)), DictReader(open(y_train)))):\n",
        "    # main training procedure\n",
        "    # step 1, get the hashed features\n",
        "    x = get_x(row, D)\n",
        "\n",
        "    # step 2, get prediction\n",
        "    p = get_p(x, w)\n",
        "    target = float(y['click'])\n",
        "\n",
        "    # for progress validation, useless for learning our model\n",
        "    loss += logloss(p, target)\n",
        "    if t % 10000 == 0 and t > 1:\n",
        "        print('%s\\tencountered: %d\\tcurrent logloss: %f' % (\n",
        "            datetime.now(), t, loss/t))\n",
        "\n",
        "    # step 3, update model with answer\n",
        "    w, n = update_w(w, n, x, p, target)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-21 15:20:36.898362\tencountered: 10000\tcurrent logloss: 0.287254\n",
            "2021-12-21 15:20:38.131796\tencountered: 20000\tcurrent logloss: 0.270824\n",
            "2021-12-21 15:20:38.950001\tencountered: 30000\tcurrent logloss: 0.265079\n",
            "2021-12-21 15:20:39.786192\tencountered: 40000\tcurrent logloss: 0.265768\n",
            "2021-12-21 15:20:40.703214\tencountered: 50000\tcurrent logloss: 0.265401\n",
            "2021-12-21 15:20:41.680085\tencountered: 60000\tcurrent logloss: 0.265177\n",
            "2021-12-21 15:20:42.748586\tencountered: 70000\tcurrent logloss: 0.264792\n",
            "2021-12-21 15:20:43.789351\tencountered: 80000\tcurrent logloss: 0.262718\n",
            "2021-12-21 15:20:44.725038\tencountered: 90000\tcurrent logloss: 0.261705\n",
            "2021-12-21 15:20:45.630974\tencountered: 100000\tcurrent logloss: 0.260590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duaypB7ydXkT"
      },
      "source": [
        "*Remark that is implementation is pure Python and quite fast while Python  is inefficient with loops without using just in time compilation. It will also handle the large dataset using a controlled memory footprint (around 200 Mo).*\n",
        "\n",
        "\n",
        "**Questions**\n",
        "1. \n",
        "Is it a good value for the loss? It may be convenient to answer such questions to compute statistics on the data. For this we are going to use numpy. If not already familiar you can find a [ref card](https://www.utc.fr/~jlaforet/Suppl/python-cheatsheets.pdf). There exist an [other one](http://mathesaurus.sourceforge.net/matlab-numpy.html) if you are already familiar with matlab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi9L4nx0dXTS",
        "outputId": "31b9295d-d174-4170-a8f2-342137d11fe3"
      },
      "source": [
        "import numpy as np\n",
        "y = np.genfromtxt('y_train.csv', delimiter=',', skip_header=1)\n",
        "#-- Answer --\n",
        "# First remark that with two classes a uniform random prediction would get is -log(1/2) ~ 0.693\n",
        "# Bur logloss of a dummy prediction always answering the average value \n",
        "y_mean = y[:,0].mean()\n",
        "Hy =  -y_mean * log( y_mean) - (1-y_mean) *log(1-y_mean)\n",
        "print( Hy )\n",
        "# ~ 0.3239\n",
        "# So 0.26 is better than random but is only better by 20% w.r.t dummy predictor "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.32391451592115916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRyjTNqEdlY5"
      },
      "source": [
        "\n",
        "\n",
        "2. What is doing the\n",
        "```\n",
        "int(value + key[4:], 16) % D\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zATJnrfztz3"
      },
      "source": [
        "**-Answer-**\n",
        "\n",
        "It is computing an index into 1..D, this is used as the memory emplacement to store the parameter corresponding to the value of $x_i$. Performances would be better with a larger value of D (default for Vopal Wabbit is $2^{24}$). This is a weak hash and it would be possible to replace it by a non cryptographic hashkey computation such as [Murmur](https://en.wikipedia.org/wiki/MurmurHash) and/or to use actual hashtable to store the values of the parameters without collision. Note that in some applications reducing the number of collisions can reduce slightly the logloss and this will always be slower.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVbr6_qJDJGG"
      },
      "source": [
        "3. Do we have an intercept term (if not how to add it) ? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXNHQziU9DpW"
      },
      "source": [
        "#Answer\n",
        "#There is no intercept in the current code.\n",
        "#See answer to question 5 for a clean addition of it using the position 0 from the array of parameters (could be any other position). This does not impact significantly the loss "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbw0Qmp7DJCZ"
      },
      "source": [
        "4. Implement a validation score using the X_valid.csv, y_valid.csv data on a fraction of test data using a  [Normalized Cross Entropy](https://www.nist.gov/system/files/documents/2017/11/30/nce.pdf). $$ NCE = (H(y) - Logloss)/H(y) $$ where H(y) is Shannon entropy of y computed in the natural basis (2 may be a better choice but as we computed the log loss in the natural basis, it is better to keep using it).\n",
        "\n",
        "Now allows to iterate several times accoss the dataset 10 times. Are you overfitting?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDdbMJifEx2k",
        "outputId": "8bceb5fd-8650-4bbb-a24c-060b3b7e4cb9"
      },
      "source": [
        "#First note that we in fact computer the entropy of y at question 1 and it is better to avoid to recompute it at each call\n",
        "#It may be better to compute it on the complete dataset but train is good enough and faster \n",
        "\n",
        "#y_data = np.genfromtxt('y_train.csv', delimiter=',', skip_header=1)\n",
        "#y_mean = y_data[:,0].mean()\n",
        "#Hy = -y_mean * log( y_mean) - (1-y_mean) *log(1-y_mean)\n",
        "\n",
        "\n",
        "X_valid = 'X_valid.csv'\n",
        "y_valid = 'y_valid.csv'\n",
        "\n",
        "D = 2 ** 20   \n",
        "alpha = .1    \n",
        "\n",
        "\n",
        "# Since this always the same data for the validation loss  we  should pin them to memory to iterate faster\n",
        "# Since the model is stored in the w we add  it as an argument but w is not usable without the function get_x and D\n",
        "# The cleanest code would decide that all of that should be stored in an object \"Model\" \n",
        "def compute_validation_loss(w, D):\n",
        "    ### TODO Complete this function ###\n",
        "    #ANSWER\n",
        "    val_loss = 0 \n",
        "    for t, (row, y)  in enumerate(zip(DictReader(open(X_valid)), DictReader(open(y_valid)))):\n",
        "      x = get_x(row, D)\n",
        "      p = get_p(x, w)\n",
        "      target = float(y['click'])\n",
        "      val_loss += logloss(p, target)\n",
        "    return val_loss/t\n",
        "\n",
        "\n",
        "w = [0.] * D  \n",
        "n = [0.] * D\n",
        "loss = 0.\n",
        "n_epochs = 10\n",
        "n_updates = 0\n",
        "training_losses = [] #To stores our training losses after each 10k updates \n",
        "validation_losses = [] #Same for validation\n",
        "\n",
        "for e in range(n_epochs):\n",
        "  training_loss = 0 #We reset to 0 the training at the begginning of each epoch to have a better idea of the current performance on the training set\n",
        "  for t, (row, y)  in enumerate(zip(DictReader(open(X_train)), DictReader(open(y_train)))):\n",
        "      x = get_x(row, D)\n",
        "      p = get_p(x, w)\n",
        "      target = float(y['click'])\n",
        "      training_loss += logloss(p, target)\n",
        "      if n_updates% 10000 == 0 and n_updates>1:\n",
        "          training_losses.append( training_loss/t )\n",
        "          validation_losses.append( compute_validation_loss(w, D) )\n",
        "          #We reuse the value of Hy computed precedently \n",
        "          print('%s\\tupdates: %d\\tcurrent logloss on train: %f\\tcurrent logloss on validation: %f \\tNCE on validation %f' % (\n",
        "              datetime.now(), n_updates, training_losses[-1], validation_losses[-1], (Hy-validation_losses[-1])/Hy ))\n",
        "          \n",
        "      w, n = update_w(w, n, x, p, target)\n",
        "      n_updates += 1\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-21 15:20:48.518950\tupdates: 10000\tcurrent logloss on train: 0.287254\tcurrent logloss on validation: 0.270539 \tNCE on validation 0.164783\n",
            "2021-12-21 15:20:49.411618\tupdates: 20000\tcurrent logloss on train: 0.270824\tcurrent logloss on validation: 0.264751 \tNCE on validation 0.182653\n",
            "2021-12-21 15:20:50.038632\tupdates: 30000\tcurrent logloss on train: 0.265079\tcurrent logloss on validation: 0.261971 \tNCE on validation 0.191235\n",
            "2021-12-21 15:20:50.662314\tupdates: 40000\tcurrent logloss on train: 0.265768\tcurrent logloss on validation: 0.259466 \tNCE on validation 0.198967\n",
            "2021-12-21 15:20:51.286401\tupdates: 50000\tcurrent logloss on train: 0.265401\tcurrent logloss on validation: 0.258067 \tNCE on validation 0.203286\n",
            "2021-12-21 15:20:51.907655\tupdates: 60000\tcurrent logloss on train: 0.265177\tcurrent logloss on validation: 0.256756 \tNCE on validation 0.207334\n",
            "2021-12-21 15:20:52.531631\tupdates: 70000\tcurrent logloss on train: 0.264792\tcurrent logloss on validation: 0.255738 \tNCE on validation 0.210477\n",
            "2021-12-21 15:20:53.167258\tupdates: 80000\tcurrent logloss on train: 0.262718\tcurrent logloss on validation: 0.255090 \tNCE on validation 0.212476\n",
            "2021-12-21 15:20:53.790755\tupdates: 90000\tcurrent logloss on train: 0.261705\tcurrent logloss on validation: 0.254110 \tNCE on validation 0.215502\n",
            "2021-12-21 15:20:54.423430\tupdates: 100000\tcurrent logloss on train: 0.260590\tcurrent logloss on validation: 0.253503 \tNCE on validation 0.217377\n",
            "2021-12-21 15:20:55.047336\tupdates: 110000\tcurrent logloss on train: 0.237529\tcurrent logloss on validation: 0.253084 \tNCE on validation 0.218670\n",
            "2021-12-21 15:20:55.682023\tupdates: 120000\tcurrent logloss on train: 0.238544\tcurrent logloss on validation: 0.252878 \tNCE on validation 0.219306\n",
            "2021-12-21 15:20:56.314347\tupdates: 130000\tcurrent logloss on train: 0.238648\tcurrent logloss on validation: 0.252633 \tNCE on validation 0.220064\n",
            "2021-12-21 15:20:56.943160\tupdates: 140000\tcurrent logloss on train: 0.243045\tcurrent logloss on validation: 0.252048 \tNCE on validation 0.221870\n",
            "2021-12-21 15:20:57.564680\tupdates: 150000\tcurrent logloss on train: 0.243551\tcurrent logloss on validation: 0.251693 \tNCE on validation 0.222965\n",
            "2021-12-21 15:20:58.178921\tupdates: 160000\tcurrent logloss on train: 0.245426\tcurrent logloss on validation: 0.251340 \tNCE on validation 0.224054\n",
            "2021-12-21 15:20:58.812361\tupdates: 170000\tcurrent logloss on train: 0.246573\tcurrent logloss on validation: 0.251033 \tNCE on validation 0.225001\n",
            "2021-12-21 15:20:59.435708\tupdates: 180000\tcurrent logloss on train: 0.246090\tcurrent logloss on validation: 0.250806 \tNCE on validation 0.225702\n",
            "2021-12-21 15:21:00.056855\tupdates: 190000\tcurrent logloss on train: 0.246010\tcurrent logloss on validation: 0.250520 \tNCE on validation 0.226585\n",
            "2021-12-21 15:21:00.660859\tupdates: 200000\tcurrent logloss on train: 0.245686\tcurrent logloss on validation: 0.250280 \tNCE on validation 0.227327\n",
            "2021-12-21 15:21:01.302715\tupdates: 210000\tcurrent logloss on train: 0.238281\tcurrent logloss on validation: 0.250143 \tNCE on validation 0.227751\n",
            "2021-12-21 15:21:01.916450\tupdates: 220000\tcurrent logloss on train: 0.235191\tcurrent logloss on validation: 0.250101 \tNCE on validation 0.227880\n",
            "2021-12-21 15:21:02.543080\tupdates: 230000\tcurrent logloss on train: 0.234934\tcurrent logloss on validation: 0.249838 \tNCE on validation 0.228690\n",
            "2021-12-21 15:21:03.170542\tupdates: 240000\tcurrent logloss on train: 0.237139\tcurrent logloss on validation: 0.249703 \tNCE on validation 0.229109\n",
            "2021-12-21 15:21:03.796818\tupdates: 250000\tcurrent logloss on train: 0.237551\tcurrent logloss on validation: 0.249538 \tNCE on validation 0.229618\n",
            "2021-12-21 15:21:04.426723\tupdates: 260000\tcurrent logloss on train: 0.240166\tcurrent logloss on validation: 0.249410 \tNCE on validation 0.230014\n",
            "2021-12-21 15:21:05.045836\tupdates: 270000\tcurrent logloss on train: 0.241633\tcurrent logloss on validation: 0.249255 \tNCE on validation 0.230491\n",
            "2021-12-21 15:21:05.674077\tupdates: 280000\tcurrent logloss on train: 0.241731\tcurrent logloss on validation: 0.249101 \tNCE on validation 0.230967\n",
            "2021-12-21 15:21:06.309896\tupdates: 290000\tcurrent logloss on train: 0.240817\tcurrent logloss on validation: 0.248925 \tNCE on validation 0.231509\n",
            "2021-12-21 15:21:06.925617\tupdates: 300000\tcurrent logloss on train: 0.240786\tcurrent logloss on validation: 0.248821 \tNCE on validation 0.231831\n",
            "2021-12-21 15:21:07.553279\tupdates: 310000\tcurrent logloss on train: 0.225544\tcurrent logloss on validation: 0.248705 \tNCE on validation 0.232189\n",
            "2021-12-21 15:21:08.183759\tupdates: 320000\tcurrent logloss on train: 0.230783\tcurrent logloss on validation: 0.248657 \tNCE on validation 0.232336\n",
            "2021-12-21 15:21:08.804975\tupdates: 330000\tcurrent logloss on train: 0.231404\tcurrent logloss on validation: 0.248539 \tNCE on validation 0.232702\n",
            "2021-12-21 15:21:09.434377\tupdates: 340000\tcurrent logloss on train: 0.232770\tcurrent logloss on validation: 0.248429 \tNCE on validation 0.233043\n",
            "2021-12-21 15:21:10.042325\tupdates: 350000\tcurrent logloss on train: 0.235049\tcurrent logloss on validation: 0.248388 \tNCE on validation 0.233167\n",
            "2021-12-21 15:21:10.671062\tupdates: 360000\tcurrent logloss on train: 0.236597\tcurrent logloss on validation: 0.248317 \tNCE on validation 0.233386\n",
            "2021-12-21 15:21:11.308045\tupdates: 370000\tcurrent logloss on train: 0.238157\tcurrent logloss on validation: 0.248200 \tNCE on validation 0.233750\n",
            "2021-12-21 15:21:11.911393\tupdates: 380000\tcurrent logloss on train: 0.238482\tcurrent logloss on validation: 0.248090 \tNCE on validation 0.234087\n",
            "2021-12-21 15:21:12.539784\tupdates: 390000\tcurrent logloss on train: 0.237827\tcurrent logloss on validation: 0.248001 \tNCE on validation 0.234362\n",
            "2021-12-21 15:21:13.160820\tupdates: 400000\tcurrent logloss on train: 0.237981\tcurrent logloss on validation: 0.247928 \tNCE on validation 0.234587\n",
            "2021-12-21 15:21:13.790876\tupdates: 410000\tcurrent logloss on train: 0.213769\tcurrent logloss on validation: 0.247798 \tNCE on validation 0.234991\n",
            "2021-12-21 15:21:14.433780\tupdates: 420000\tcurrent logloss on train: 0.234269\tcurrent logloss on validation: 0.247798 \tNCE on validation 0.234989\n",
            "2021-12-21 15:21:15.049180\tupdates: 430000\tcurrent logloss on train: 0.228719\tcurrent logloss on validation: 0.247765 \tNCE on validation 0.235091\n",
            "2021-12-21 15:21:15.677452\tupdates: 440000\tcurrent logloss on train: 0.227963\tcurrent logloss on validation: 0.247783 \tNCE on validation 0.235036\n",
            "2021-12-21 15:21:16.315916\tupdates: 450000\tcurrent logloss on train: 0.231966\tcurrent logloss on validation: 0.247609 \tNCE on validation 0.235572\n",
            "2021-12-21 15:21:16.963333\tupdates: 460000\tcurrent logloss on train: 0.233478\tcurrent logloss on validation: 0.247650 \tNCE on validation 0.235446\n",
            "2021-12-21 15:21:17.592069\tupdates: 470000\tcurrent logloss on train: 0.235316\tcurrent logloss on validation: 0.247553 \tNCE on validation 0.235746\n",
            "2021-12-21 15:21:18.204807\tupdates: 480000\tcurrent logloss on train: 0.236021\tcurrent logloss on validation: 0.247458 \tNCE on validation 0.236041\n",
            "2021-12-21 15:21:18.844586\tupdates: 490000\tcurrent logloss on train: 0.235435\tcurrent logloss on validation: 0.247432 \tNCE on validation 0.236120\n",
            "2021-12-21 15:21:19.475241\tupdates: 500000\tcurrent logloss on train: 0.235581\tcurrent logloss on validation: 0.247310 \tNCE on validation 0.236496\n",
            "2021-12-21 15:21:20.089240\tupdates: 510000\tcurrent logloss on train: 0.235563\tcurrent logloss on validation: 0.247251 \tNCE on validation 0.236677\n",
            "2021-12-21 15:21:20.711405\tupdates: 520000\tcurrent logloss on train: 0.227238\tcurrent logloss on validation: 0.247202 \tNCE on validation 0.236831\n",
            "2021-12-21 15:21:21.326918\tupdates: 530000\tcurrent logloss on train: 0.226341\tcurrent logloss on validation: 0.247252 \tNCE on validation 0.236675\n",
            "2021-12-21 15:21:21.961047\tupdates: 540000\tcurrent logloss on train: 0.226299\tcurrent logloss on validation: 0.247266 \tNCE on validation 0.236631\n",
            "2021-12-21 15:21:22.601521\tupdates: 550000\tcurrent logloss on train: 0.230462\tcurrent logloss on validation: 0.247137 \tNCE on validation 0.237031\n",
            "2021-12-21 15:21:23.218100\tupdates: 560000\tcurrent logloss on train: 0.230912\tcurrent logloss on validation: 0.247103 \tNCE on validation 0.237135\n",
            "2021-12-21 15:21:23.850661\tupdates: 570000\tcurrent logloss on train: 0.233196\tcurrent logloss on validation: 0.247072 \tNCE on validation 0.237230\n",
            "2021-12-21 15:21:24.465602\tupdates: 580000\tcurrent logloss on train: 0.234426\tcurrent logloss on validation: 0.247025 \tNCE on validation 0.237377\n",
            "2021-12-21 15:21:25.102241\tupdates: 590000\tcurrent logloss on train: 0.233645\tcurrent logloss on validation: 0.246962 \tNCE on validation 0.237571\n",
            "2021-12-21 15:21:25.731523\tupdates: 600000\tcurrent logloss on train: 0.233772\tcurrent logloss on validation: 0.246899 \tNCE on validation 0.237765\n",
            "2021-12-21 15:21:26.350454\tupdates: 610000\tcurrent logloss on train: 0.233713\tcurrent logloss on validation: 0.246848 \tNCE on validation 0.237923\n",
            "2021-12-21 15:21:26.979236\tupdates: 620000\tcurrent logloss on train: 0.229271\tcurrent logloss on validation: 0.246833 \tNCE on validation 0.237968\n",
            "2021-12-21 15:21:27.613928\tupdates: 630000\tcurrent logloss on train: 0.226614\tcurrent logloss on validation: 0.246867 \tNCE on validation 0.237865\n",
            "2021-12-21 15:21:28.240324\tupdates: 640000\tcurrent logloss on train: 0.225520\tcurrent logloss on validation: 0.246799 \tNCE on validation 0.238074\n",
            "2021-12-21 15:21:28.864575\tupdates: 650000\tcurrent logloss on train: 0.228533\tcurrent logloss on validation: 0.246767 \tNCE on validation 0.238174\n",
            "2021-12-21 15:21:29.485606\tupdates: 660000\tcurrent logloss on train: 0.228884\tcurrent logloss on validation: 0.246768 \tNCE on validation 0.238168\n",
            "2021-12-21 15:21:30.116326\tupdates: 670000\tcurrent logloss on train: 0.230977\tcurrent logloss on validation: 0.246741 \tNCE on validation 0.238252\n",
            "2021-12-21 15:21:30.745947\tupdates: 680000\tcurrent logloss on train: 0.232789\tcurrent logloss on validation: 0.246694 \tNCE on validation 0.238398\n",
            "2021-12-21 15:21:31.364634\tupdates: 690000\tcurrent logloss on train: 0.232805\tcurrent logloss on validation: 0.246660 \tNCE on validation 0.238504\n",
            "2021-12-21 15:21:31.985954\tupdates: 700000\tcurrent logloss on train: 0.232038\tcurrent logloss on validation: 0.246593 \tNCE on validation 0.238711\n",
            "2021-12-21 15:21:32.614980\tupdates: 710000\tcurrent logloss on train: 0.232197\tcurrent logloss on validation: 0.246570 \tNCE on validation 0.238781\n",
            "2021-12-21 15:21:33.243748\tupdates: 720000\tcurrent logloss on train: 0.223088\tcurrent logloss on validation: 0.246537 \tNCE on validation 0.238882\n",
            "2021-12-21 15:21:33.880603\tupdates: 730000\tcurrent logloss on train: 0.223746\tcurrent logloss on validation: 0.246543 \tNCE on validation 0.238865\n",
            "2021-12-21 15:21:34.507975\tupdates: 740000\tcurrent logloss on train: 0.224634\tcurrent logloss on validation: 0.246508 \tNCE on validation 0.238972\n",
            "2021-12-21 15:21:35.137400\tupdates: 750000\tcurrent logloss on train: 0.225959\tcurrent logloss on validation: 0.246475 \tNCE on validation 0.239074\n",
            "2021-12-21 15:21:35.762113\tupdates: 760000\tcurrent logloss on train: 0.227947\tcurrent logloss on validation: 0.246485 \tNCE on validation 0.239042\n",
            "2021-12-21 15:21:36.373729\tupdates: 770000\tcurrent logloss on train: 0.229502\tcurrent logloss on validation: 0.246486 \tNCE on validation 0.239040\n",
            "2021-12-21 15:21:37.004522\tupdates: 780000\tcurrent logloss on train: 0.231095\tcurrent logloss on validation: 0.246435 \tNCE on validation 0.239198\n",
            "2021-12-21 15:21:37.632485\tupdates: 790000\tcurrent logloss on train: 0.231416\tcurrent logloss on validation: 0.246407 \tNCE on validation 0.239285\n",
            "2021-12-21 15:21:38.257473\tupdates: 800000\tcurrent logloss on train: 0.230758\tcurrent logloss on validation: 0.246358 \tNCE on validation 0.239434\n",
            "2021-12-21 15:21:38.882913\tupdates: 810000\tcurrent logloss on train: 0.231092\tcurrent logloss on validation: 0.246344 \tNCE on validation 0.239479\n",
            "2021-12-21 15:21:39.503108\tupdates: 820000\tcurrent logloss on train: 0.244233\tcurrent logloss on validation: 0.246298 \tNCE on validation 0.239622\n",
            "2021-12-21 15:21:40.137495\tupdates: 830000\tcurrent logloss on train: 0.228140\tcurrent logloss on validation: 0.246305 \tNCE on validation 0.239597\n",
            "2021-12-21 15:21:40.754020\tupdates: 840000\tcurrent logloss on train: 0.223423\tcurrent logloss on validation: 0.246305 \tNCE on validation 0.239598\n",
            "2021-12-21 15:21:41.387534\tupdates: 850000\tcurrent logloss on train: 0.222693\tcurrent logloss on validation: 0.246326 \tNCE on validation 0.239534\n",
            "2021-12-21 15:21:42.015897\tupdates: 860000\tcurrent logloss on train: 0.225968\tcurrent logloss on validation: 0.246248 \tNCE on validation 0.239776\n",
            "2021-12-21 15:21:42.625032\tupdates: 870000\tcurrent logloss on train: 0.227534\tcurrent logloss on validation: 0.246298 \tNCE on validation 0.239622\n",
            "2021-12-21 15:21:43.252070\tupdates: 880000\tcurrent logloss on train: 0.229390\tcurrent logloss on validation: 0.246261 \tNCE on validation 0.239733\n",
            "2021-12-21 15:21:43.876485\tupdates: 890000\tcurrent logloss on train: 0.230098\tcurrent logloss on validation: 0.246238 \tNCE on validation 0.239806\n",
            "2021-12-21 15:21:44.492102\tupdates: 900000\tcurrent logloss on train: 0.229280\tcurrent logloss on validation: 0.246230 \tNCE on validation 0.239829\n",
            "2021-12-21 15:21:45.124489\tupdates: 910000\tcurrent logloss on train: 0.229793\tcurrent logloss on validation: 0.246175 \tNCE on validation 0.239999\n",
            "2021-12-21 15:21:45.746631\tupdates: 920000\tcurrent logloss on train: 0.229733\tcurrent logloss on validation: 0.246136 \tNCE on validation 0.240119\n",
            "2021-12-21 15:21:46.372819\tupdates: 930000\tcurrent logloss on train: 0.222926\tcurrent logloss on validation: 0.246126 \tNCE on validation 0.240152\n",
            "2021-12-21 15:21:47.009180\tupdates: 940000\tcurrent logloss on train: 0.221634\tcurrent logloss on validation: 0.246166 \tNCE on validation 0.240027\n",
            "2021-12-21 15:21:47.628528\tupdates: 950000\tcurrent logloss on train: 0.221079\tcurrent logloss on validation: 0.246199 \tNCE on validation 0.239926\n",
            "2021-12-21 15:21:48.251568\tupdates: 960000\tcurrent logloss on train: 0.225309\tcurrent logloss on validation: 0.246128 \tNCE on validation 0.240144\n",
            "2021-12-21 15:21:48.868888\tupdates: 970000\tcurrent logloss on train: 0.225815\tcurrent logloss on validation: 0.246113 \tNCE on validation 0.240191\n",
            "2021-12-21 15:21:49.494594\tupdates: 980000\tcurrent logloss on train: 0.228065\tcurrent logloss on validation: 0.246119 \tNCE on validation 0.240173\n",
            "2021-12-21 15:21:50.120656\tupdates: 990000\tcurrent logloss on train: 0.229335\tcurrent logloss on validation: 0.246106 \tNCE on validation 0.240213\n",
            "2021-12-21 15:21:50.736274\tupdates: 1000000\tcurrent logloss on train: 0.228537\tcurrent logloss on validation: 0.246077 \tNCE on validation 0.240303\n",
            "2021-12-21 15:21:51.358237\tupdates: 1010000\tcurrent logloss on train: 0.228706\tcurrent logloss on validation: 0.246035 \tNCE on validation 0.240434\n",
            "2021-12-21 15:21:51.978844\tupdates: 1020000\tcurrent logloss on train: 0.228641\tcurrent logloss on validation: 0.246017 \tNCE on validation 0.240486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "hi4NZ_a4H0Oy",
        "outputId": "2cb1870e-9025-45f7-c140-8a52a68cd282"
      },
      "source": [
        "#Produce a plot of the losses\n",
        "#ANSWER\n",
        "import matplotlib.pyplot as plt\n",
        "#from matplotlib.pyplot import figure\n",
        "#figure(figsize=(4, 3), dpi=150) #Colab does only support inline display but you can change figure size with this\n",
        "\n",
        "x = [10000*i for i in range(len(training_losses))]\n",
        "plt.plot(x, training_losses, label='Train')\n",
        "plt.plot(x, validation_losses, label='Validation')\n",
        "plt.xlabel('Number of updates')\n",
        "plt.ylabel('Log Loss')\n",
        "plt.legend( ('Train', 'Validation') )\n",
        "plt.show()\n",
        "# overfitting, the bumps on the training loss are due to the reset after each epoch. => Not so good to reset the estimation of the loss  \n",
        "#Log loss is ploted NCE is fine too (anyway this is only a rescaling)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_CPkIovFIPg"
      },
      "source": [
        "5. Modify the code to enable the use of all quadratic features. This is doable only by modifying the function ```get_x``` \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVcD_0PnevRB"
      },
      "source": [
        "#ANSWER with ony one vector for parameters. This is also possible to split the first and second order terms\n",
        "def get_x(csv_row, D):\n",
        "     x = []\n",
        "     # normal features\n",
        "     for key, value in csv_row.items():\n",
        "         index = int(value + key[4:], 16) % D  \n",
        "         x.append(index)\n",
        "     # second order terms\n",
        "     L = len(x)\n",
        "     for i in range(L):\n",
        "         for j in range(i+1, L):\n",
        "             index = (x[i] * x[j]) % D  # second weakest hash\n",
        "             x.append(index)\n",
        "\n",
        "     # bias\n",
        "     x.append(0)\n",
        "\n",
        "     return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw9nXonCevel"
      },
      "source": [
        "4. After how many updates are you overfitting ? Same question after adding a L2 regularisation term with ponderation $5\\cdot 10^{-4}$ and/or with a the learning rate of 0.01. (side question should we include the bias term in the regularization ?)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUxq-KU_FIE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b09b0d-cdc4-41f7-babd-5c8868f2e064"
      },
      "source": [
        "w = [0.] * D  \n",
        "n = [0.] * D\n",
        "loss = 0.\n",
        "n_epochs = 10\n",
        "n_updates = 0\n",
        "training_losses = [] #To stores our training losses after each 10k updates \n",
        "validation_losses = [] #Same for validation\n",
        "\n",
        "for e in range(n_epochs):\n",
        "  training_loss = 0 #We reset to 0 the training at the begginning of each epoch to have a better idea of the current performance on the training set\n",
        "  for t, (row, y)  in enumerate(zip(DictReader(open(X_train)), DictReader(open(y_train)))):\n",
        "      x = get_x(row, D)\n",
        "      p = get_p(x, w)\n",
        "      target = float(y['click'])\n",
        "      training_loss += logloss(p, target)\n",
        "      if n_updates% 10000 == 0 and n_updates>1:\n",
        "          training_losses.append( training_loss/t )\n",
        "          validation_losses.append( compute_validation_loss(w, D) )\n",
        "          print('%s\\tupdates: %d\\tcurrent logloss on train: %f\\tcurrent logloss on validation: %f \\tNCE in validation %f' % (\n",
        "              datetime.now(), n_updates, training_losses[-1], validation_losses[-1], (Hy-validation_losses[-1])/Hy ))\n",
        "      w, n = update_w(w, n, x, p, target)\n",
        "      n_updates += 1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-21 15:21:55.791737\tupdates: 10000\tcurrent logloss on train: 0.291079\tcurrent logloss on validation: 0.269238 \tNCE in validation 0.168801\n",
            "2021-12-21 15:21:59.173978\tupdates: 20000\tcurrent logloss on train: 0.275756\tcurrent logloss on validation: 0.262381 \tNCE in validation 0.189969\n",
            "2021-12-21 15:22:02.581319\tupdates: 30000\tcurrent logloss on train: 0.270412\tcurrent logloss on validation: 0.261039 \tNCE in validation 0.194110\n",
            "2021-12-21 15:22:06.034717\tupdates: 40000\tcurrent logloss on train: 0.270709\tcurrent logloss on validation: 0.259100 \tNCE in validation 0.200096\n",
            "2021-12-21 15:22:09.446068\tupdates: 50000\tcurrent logloss on train: 0.269615\tcurrent logloss on validation: 0.259220 \tNCE in validation 0.199727\n",
            "2021-12-21 15:22:12.864608\tupdates: 60000\tcurrent logloss on train: 0.269340\tcurrent logloss on validation: 0.258151 \tNCE in validation 0.203027\n",
            "2021-12-21 15:22:16.265682\tupdates: 70000\tcurrent logloss on train: 0.268499\tcurrent logloss on validation: 0.256990 \tNCE in validation 0.206612\n",
            "2021-12-21 15:22:19.677291\tupdates: 80000\tcurrent logloss on train: 0.266172\tcurrent logloss on validation: 0.256852 \tNCE in validation 0.207038\n",
            "2021-12-21 15:22:23.063424\tupdates: 90000\tcurrent logloss on train: 0.264738\tcurrent logloss on validation: 0.254087 \tNCE in validation 0.215574\n",
            "2021-12-21 15:22:26.884418\tupdates: 100000\tcurrent logloss on train: 0.263739\tcurrent logloss on validation: 0.253509 \tNCE in validation 0.217358\n",
            "2021-12-21 15:22:31.167983\tupdates: 110000\tcurrent logloss on train: 0.082790\tcurrent logloss on validation: 0.254919 \tNCE in validation 0.213006\n",
            "2021-12-21 15:22:34.559718\tupdates: 120000\tcurrent logloss on train: 0.104013\tcurrent logloss on validation: 0.254715 \tNCE in validation 0.213636\n",
            "2021-12-21 15:22:37.968621\tupdates: 130000\tcurrent logloss on train: 0.116262\tcurrent logloss on validation: 0.255177 \tNCE in validation 0.212210\n",
            "2021-12-21 15:22:41.372796\tupdates: 140000\tcurrent logloss on train: 0.127401\tcurrent logloss on validation: 0.255091 \tNCE in validation 0.212474\n",
            "2021-12-21 15:22:44.756235\tupdates: 150000\tcurrent logloss on train: 0.134056\tcurrent logloss on validation: 0.256312 \tNCE in validation 0.208706\n",
            "2021-12-21 15:22:48.173736\tupdates: 160000\tcurrent logloss on train: 0.141087\tcurrent logloss on validation: 0.256302 \tNCE in validation 0.208735\n",
            "2021-12-21 15:22:51.559732\tupdates: 170000\tcurrent logloss on train: 0.146106\tcurrent logloss on validation: 0.256384 \tNCE in validation 0.208482\n",
            "2021-12-21 15:22:54.977992\tupdates: 180000\tcurrent logloss on train: 0.149404\tcurrent logloss on validation: 0.257162 \tNCE in validation 0.206081\n",
            "2021-12-21 15:22:58.410137\tupdates: 190000\tcurrent logloss on train: 0.152409\tcurrent logloss on validation: 0.256329 \tNCE in validation 0.208654\n",
            "2021-12-21 15:23:01.824367\tupdates: 200000\tcurrent logloss on train: 0.155480\tcurrent logloss on validation: 0.256356 \tNCE in validation 0.208570\n",
            "2021-12-21 15:23:05.223759\tupdates: 210000\tcurrent logloss on train: 0.070614\tcurrent logloss on validation: 0.257721 \tNCE in validation 0.204354\n",
            "2021-12-21 15:23:08.655235\tupdates: 220000\tcurrent logloss on train: 0.089958\tcurrent logloss on validation: 0.258793 \tNCE in validation 0.201046\n",
            "2021-12-21 15:23:12.089837\tupdates: 230000\tcurrent logloss on train: 0.098609\tcurrent logloss on validation: 0.257045 \tNCE in validation 0.206441\n",
            "2021-12-21 15:23:15.532984\tupdates: 240000\tcurrent logloss on train: 0.106467\tcurrent logloss on validation: 0.257520 \tNCE in validation 0.204974\n",
            "2021-12-21 15:23:18.950452\tupdates: 250000\tcurrent logloss on train: 0.111405\tcurrent logloss on validation: 0.258440 \tNCE in validation 0.202135\n",
            "2021-12-21 15:23:22.399083\tupdates: 260000\tcurrent logloss on train: 0.117270\tcurrent logloss on validation: 0.259152 \tNCE in validation 0.199937\n",
            "2021-12-21 15:23:25.835834\tupdates: 270000\tcurrent logloss on train: 0.121509\tcurrent logloss on validation: 0.259218 \tNCE in validation 0.199732\n",
            "2021-12-21 15:23:29.316903\tupdates: 280000\tcurrent logloss on train: 0.124228\tcurrent logloss on validation: 0.259384 \tNCE in validation 0.199221\n",
            "2021-12-21 15:23:32.741886\tupdates: 290000\tcurrent logloss on train: 0.126190\tcurrent logloss on validation: 0.259242 \tNCE in validation 0.199660\n",
            "2021-12-21 15:23:36.169980\tupdates: 300000\tcurrent logloss on train: 0.128702\tcurrent logloss on validation: 0.259369 \tNCE in validation 0.199266\n",
            "2021-12-21 15:23:39.568971\tupdates: 310000\tcurrent logloss on train: 0.056635\tcurrent logloss on validation: 0.261924 \tNCE in validation 0.191380\n",
            "2021-12-21 15:23:42.996770\tupdates: 320000\tcurrent logloss on train: 0.078948\tcurrent logloss on validation: 0.262107 \tNCE in validation 0.190813\n",
            "2021-12-21 15:23:46.424301\tupdates: 330000\tcurrent logloss on train: 0.087818\tcurrent logloss on validation: 0.260168 \tNCE in validation 0.196799\n",
            "2021-12-21 15:23:49.852740\tupdates: 340000\tcurrent logloss on train: 0.094030\tcurrent logloss on validation: 0.259990 \tNCE in validation 0.197349\n",
            "2021-12-21 15:23:53.306609\tupdates: 350000\tcurrent logloss on train: 0.098940\tcurrent logloss on validation: 0.260945 \tNCE in validation 0.194402\n",
            "2021-12-21 15:23:56.716910\tupdates: 360000\tcurrent logloss on train: 0.103238\tcurrent logloss on validation: 0.261971 \tNCE in validation 0.191233\n",
            "2021-12-21 15:24:00.118068\tupdates: 370000\tcurrent logloss on train: 0.107132\tcurrent logloss on validation: 0.261937 \tNCE in validation 0.191340\n",
            "2021-12-21 15:24:03.508955\tupdates: 380000\tcurrent logloss on train: 0.109490\tcurrent logloss on validation: 0.262130 \tNCE in validation 0.190743\n",
            "2021-12-21 15:24:06.892794\tupdates: 390000\tcurrent logloss on train: 0.111198\tcurrent logloss on validation: 0.262319 \tNCE in validation 0.190158\n",
            "2021-12-21 15:24:10.308851\tupdates: 400000\tcurrent logloss on train: 0.113106\tcurrent logloss on validation: 0.262243 \tNCE in validation 0.190394\n",
            "2021-12-21 15:24:13.734287\tupdates: 410000\tcurrent logloss on train: 0.042850\tcurrent logloss on validation: 0.262278 \tNCE in validation 0.190285\n",
            "2021-12-21 15:24:17.145746\tupdates: 420000\tcurrent logloss on train: 0.073430\tcurrent logloss on validation: 0.263469 \tNCE in validation 0.186610\n",
            "2021-12-21 15:24:20.561271\tupdates: 430000\tcurrent logloss on train: 0.079503\tcurrent logloss on validation: 0.263770 \tNCE in validation 0.185681\n",
            "2021-12-21 15:24:23.983382\tupdates: 440000\tcurrent logloss on train: 0.084493\tcurrent logloss on validation: 0.264481 \tNCE in validation 0.183486\n",
            "2021-12-21 15:24:27.400889\tupdates: 450000\tcurrent logloss on train: 0.089738\tcurrent logloss on validation: 0.263453 \tNCE in validation 0.186660\n",
            "2021-12-21 15:24:30.895511\tupdates: 460000\tcurrent logloss on train: 0.093382\tcurrent logloss on validation: 0.264126 \tNCE in validation 0.184582\n",
            "2021-12-21 15:24:34.328210\tupdates: 470000\tcurrent logloss on train: 0.096911\tcurrent logloss on validation: 0.264634 \tNCE in validation 0.183013\n",
            "2021-12-21 15:24:37.744222\tupdates: 480000\tcurrent logloss on train: 0.099303\tcurrent logloss on validation: 0.264660 \tNCE in validation 0.182932\n",
            "2021-12-21 15:24:41.180174\tupdates: 490000\tcurrent logloss on train: 0.100564\tcurrent logloss on validation: 0.265699 \tNCE in validation 0.179724\n",
            "2021-12-21 15:24:44.586956\tupdates: 500000\tcurrent logloss on train: 0.102256\tcurrent logloss on validation: 0.264775 \tNCE in validation 0.182579\n",
            "2021-12-21 15:24:47.993370\tupdates: 510000\tcurrent logloss on train: 0.103972\tcurrent logloss on validation: 0.264869 \tNCE in validation 0.182287\n",
            "2021-12-21 15:24:51.419539\tupdates: 520000\tcurrent logloss on train: 0.064607\tcurrent logloss on validation: 0.266923 \tNCE in validation 0.175946\n",
            "2021-12-21 15:24:54.834072\tupdates: 530000\tcurrent logloss on train: 0.073336\tcurrent logloss on validation: 0.267625 \tNCE in validation 0.173780\n",
            "2021-12-21 15:24:58.276251\tupdates: 540000\tcurrent logloss on train: 0.078056\tcurrent logloss on validation: 0.267068 \tNCE in validation 0.175500\n",
            "2021-12-21 15:25:01.741735\tupdates: 550000\tcurrent logloss on train: 0.083151\tcurrent logloss on validation: 0.265899 \tNCE in validation 0.179108\n",
            "2021-12-21 15:25:05.168624\tupdates: 560000\tcurrent logloss on train: 0.085921\tcurrent logloss on validation: 0.266222 \tNCE in validation 0.178111\n",
            "2021-12-21 15:25:08.636628\tupdates: 570000\tcurrent logloss on train: 0.089296\tcurrent logloss on validation: 0.266767 \tNCE in validation 0.176428\n",
            "2021-12-21 15:25:12.039201\tupdates: 580000\tcurrent logloss on train: 0.091726\tcurrent logloss on validation: 0.266649 \tNCE in validation 0.176793\n",
            "2021-12-21 15:25:15.477502\tupdates: 590000\tcurrent logloss on train: 0.092816\tcurrent logloss on validation: 0.267574 \tNCE in validation 0.173936\n",
            "2021-12-21 15:25:18.912342\tupdates: 600000\tcurrent logloss on train: 0.094143\tcurrent logloss on validation: 0.266993 \tNCE in validation 0.175729\n",
            "2021-12-21 15:25:22.374760\tupdates: 610000\tcurrent logloss on train: 0.095650\tcurrent logloss on validation: 0.267046 \tNCE in validation 0.175567\n",
            "2021-12-21 15:25:25.807867\tupdates: 620000\tcurrent logloss on train: 0.059571\tcurrent logloss on validation: 0.268641 \tNCE in validation 0.170642\n",
            "2021-12-21 15:25:29.263626\tupdates: 630000\tcurrent logloss on train: 0.069418\tcurrent logloss on validation: 0.270239 \tNCE in validation 0.165709\n",
            "2021-12-21 15:25:32.719098\tupdates: 640000\tcurrent logloss on train: 0.073278\tcurrent logloss on validation: 0.268811 \tNCE in validation 0.170117\n",
            "2021-12-21 15:25:36.190043\tupdates: 650000\tcurrent logloss on train: 0.077652\tcurrent logloss on validation: 0.268070 \tNCE in validation 0.172404\n",
            "2021-12-21 15:25:39.638891\tupdates: 660000\tcurrent logloss on train: 0.080120\tcurrent logloss on validation: 0.268598 \tNCE in validation 0.170774\n",
            "2021-12-21 15:25:43.092822\tupdates: 670000\tcurrent logloss on train: 0.083255\tcurrent logloss on validation: 0.269255 \tNCE in validation 0.168746\n",
            "2021-12-21 15:25:46.546743\tupdates: 680000\tcurrent logloss on train: 0.085523\tcurrent logloss on validation: 0.268915 \tNCE in validation 0.169797\n",
            "2021-12-21 15:25:49.966122\tupdates: 690000\tcurrent logloss on train: 0.086835\tcurrent logloss on validation: 0.269043 \tNCE in validation 0.169402\n",
            "2021-12-21 15:25:53.390393\tupdates: 700000\tcurrent logloss on train: 0.087712\tcurrent logloss on validation: 0.269603 \tNCE in validation 0.167672\n",
            "2021-12-21 15:25:56.825942\tupdates: 710000\tcurrent logloss on train: 0.089120\tcurrent logloss on validation: 0.269233 \tNCE in validation 0.168813\n",
            "2021-12-21 15:26:00.240749\tupdates: 720000\tcurrent logloss on train: 0.051256\tcurrent logloss on validation: 0.270868 \tNCE in validation 0.163766\n",
            "2021-12-21 15:26:03.623240\tupdates: 730000\tcurrent logloss on train: 0.064343\tcurrent logloss on validation: 0.272293 \tNCE in validation 0.159369\n",
            "2021-12-21 15:26:07.040946\tupdates: 740000\tcurrent logloss on train: 0.069290\tcurrent logloss on validation: 0.270873 \tNCE in validation 0.163751\n",
            "2021-12-21 15:26:10.435822\tupdates: 750000\tcurrent logloss on train: 0.072859\tcurrent logloss on validation: 0.270360 \tNCE in validation 0.165336\n",
            "2021-12-21 15:26:13.858348\tupdates: 760000\tcurrent logloss on train: 0.075538\tcurrent logloss on validation: 0.270821 \tNCE in validation 0.163914\n",
            "2021-12-21 15:26:17.259608\tupdates: 770000\tcurrent logloss on train: 0.078217\tcurrent logloss on validation: 0.271172 \tNCE in validation 0.162828\n",
            "2021-12-21 15:26:20.673843\tupdates: 780000\tcurrent logloss on train: 0.080500\tcurrent logloss on validation: 0.270943 \tNCE in validation 0.163534\n",
            "2021-12-21 15:26:24.091104\tupdates: 790000\tcurrent logloss on train: 0.081861\tcurrent logloss on validation: 0.271277 \tNCE in validation 0.162504\n",
            "2021-12-21 15:26:27.532316\tupdates: 800000\tcurrent logloss on train: 0.082674\tcurrent logloss on validation: 0.272012 \tNCE in validation 0.160236\n",
            "2021-12-21 15:26:30.964059\tupdates: 810000\tcurrent logloss on train: 0.083747\tcurrent logloss on validation: 0.271436 \tNCE in validation 0.162014\n",
            "2021-12-21 15:26:34.420179\tupdates: 820000\tcurrent logloss on train: 0.044706\tcurrent logloss on validation: 0.271504 \tNCE in validation 0.161803\n",
            "2021-12-21 15:26:37.838068\tupdates: 830000\tcurrent logloss on train: 0.061663\tcurrent logloss on validation: 0.273068 \tNCE in validation 0.156975\n",
            "2021-12-21 15:26:41.233652\tupdates: 840000\tcurrent logloss on train: 0.065417\tcurrent logloss on validation: 0.273547 \tNCE in validation 0.155497\n",
            "2021-12-21 15:26:44.649645\tupdates: 850000\tcurrent logloss on train: 0.068197\tcurrent logloss on validation: 0.273836 \tNCE in validation 0.154603\n",
            "2021-12-21 15:26:48.038445\tupdates: 860000\tcurrent logloss on train: 0.071421\tcurrent logloss on validation: 0.272632 \tNCE in validation 0.158321\n",
            "2021-12-21 15:26:51.452313\tupdates: 870000\tcurrent logloss on train: 0.073810\tcurrent logloss on validation: 0.272608 \tNCE in validation 0.158395\n",
            "2021-12-21 15:26:54.904059\tupdates: 880000\tcurrent logloss on train: 0.076068\tcurrent logloss on validation: 0.273238 \tNCE in validation 0.156450\n",
            "2021-12-21 15:26:58.323307\tupdates: 890000\tcurrent logloss on train: 0.077583\tcurrent logloss on validation: 0.273230 \tNCE in validation 0.156474\n",
            "2021-12-21 15:27:01.762247\tupdates: 900000\tcurrent logloss on train: 0.078086\tcurrent logloss on validation: 0.274521 \tNCE in validation 0.152490\n",
            "2021-12-21 15:27:05.168959\tupdates: 910000\tcurrent logloss on train: 0.079250\tcurrent logloss on validation: 0.273532 \tNCE in validation 0.155543\n",
            "2021-12-21 15:27:08.601602\tupdates: 920000\tcurrent logloss on train: 0.080286\tcurrent logloss on validation: 0.273393 \tNCE in validation 0.155972\n",
            "2021-12-21 15:27:12.047459\tupdates: 930000\tcurrent logloss on train: 0.056355\tcurrent logloss on validation: 0.275282 \tNCE in validation 0.150141\n",
            "2021-12-21 15:27:15.483099\tupdates: 940000\tcurrent logloss on train: 0.061984\tcurrent logloss on validation: 0.276435 \tNCE in validation 0.146579\n",
            "2021-12-21 15:27:18.896485\tupdates: 950000\tcurrent logloss on train: 0.064551\tcurrent logloss on validation: 0.275978 \tNCE in validation 0.147992\n",
            "2021-12-21 15:27:22.340858\tupdates: 960000\tcurrent logloss on train: 0.068203\tcurrent logloss on validation: 0.274544 \tNCE in validation 0.152420\n",
            "2021-12-21 15:27:25.762377\tupdates: 970000\tcurrent logloss on train: 0.070045\tcurrent logloss on validation: 0.274340 \tNCE in validation 0.153049\n",
            "2021-12-21 15:27:29.178052\tupdates: 980000\tcurrent logloss on train: 0.072344\tcurrent logloss on validation: 0.274865 \tNCE in validation 0.151426\n",
            "2021-12-21 15:27:32.604620\tupdates: 990000\tcurrent logloss on train: 0.073972\tcurrent logloss on validation: 0.274629 \tNCE in validation 0.152156\n",
            "2021-12-21 15:27:36.021376\tupdates: 1000000\tcurrent logloss on train: 0.074489\tcurrent logloss on validation: 0.275751 \tNCE in validation 0.148691\n",
            "2021-12-21 15:27:39.439164\tupdates: 1010000\tcurrent logloss on train: 0.075348\tcurrent logloss on validation: 0.275181 \tNCE in validation 0.150452\n",
            "2021-12-21 15:27:42.858149\tupdates: 1020000\tcurrent logloss on train: 0.076333\tcurrent logloss on validation: 0.274981 \tNCE in validation 0.151069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "STKNAlpTNBU-",
        "outputId": "aba32b38-f0c9-4d16-f5cc-4ac4425ef9db"
      },
      "source": [
        "#Produce a plot of the losses\n",
        "#ANSWER\n",
        "x = [10000*i for i in range(len(training_losses))]\n",
        "plt.plot(x, training_losses, label='Train')\n",
        "plt.plot(x, validation_losses, label='Validation')\n",
        "plt.xlabel('Number of updates')\n",
        "plt.ylabel('Log Loss')\n",
        "plt.legend( ('Train', 'Validation') )\n",
        "plt.show()\n",
        "#Clear overfitting.\n",
        "#L2 regularization should help. Smaller step size will help in terms of number of updates but is possibly just slowing the process"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "yARCrLCyOIgy",
        "outputId": "7b4e890c-1f2c-4084-cac8-fe85a546c9bf"
      },
      "source": [
        "#ANSWER - same stuff with a L2 regularizer (bias term + quad terms)\n",
        "#A lot of settings allows a large difference between train and validation, theses are indeed bad\n",
        "#Remark than combining L1 and L2 is easy and is often named \"elasticnet\"\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "def update_wL2(w, n, x, p, y, lbd=5e-4):\n",
        "    for i in x:\n",
        "        # L2 reg added here. \n",
        "        w[i] -= ((p - y + lbd * 2 * w[i])  ) * alpha / (sqrt(n[i]) + 1.) \n",
        "        n[i] += 1.\n",
        "    return w, n\n",
        "\n",
        "sign = lambda x: math.copysign(1, x) #python has cumbersome sign function \n",
        "def update_wL1(w, n, x, p, y, lbd=5e-4):\n",
        "    for i in x:\n",
        "        w[i] -= ((p - y + ) * 2 + lbd * sign(w[i])) * alpha / (sqrt(n[i]) + 1.) \n",
        "        n[i] += 1.\n",
        "    return w, n\n",
        "\n",
        "D = 2 ** 24\n",
        "w = [0.] * D  \n",
        "n = [0.] * D\n",
        "loss = 0.\n",
        "n_epochs = 10\n",
        "n_updates = 0\n",
        "training_losses = [] \n",
        "validation_losses = [] \n",
        "alpha = .01 #CARE THE LEARNING RATE HAS BEEN MODIFIED \n",
        "\n",
        "for e in range(n_epochs):\n",
        "  training_loss = 0\n",
        "  for t, (row, y)  in enumerate(zip(DictReader(open(X_train)), DictReader(open(y_train)))):\n",
        "      x = get_x(row, D)\n",
        "      p = get_p(x, w)\n",
        "      target = float(y['click'])\n",
        "      training_loss += logloss(p, target)\n",
        "      if n_updates% 10000 == 0 and n_updates>1:\n",
        "          training_losses.append( training_loss/t )\n",
        "          validation_losses.append( compute_validation_loss(w, D) )\n",
        "          print('%s\\tupdates: %d\\tcurrent logloss on train: %f\\tcurrent logloss on validation: %f \\tNCE in validation %f' % (\n",
        "              datetime.now(), n_updates, training_losses[-1], validation_losses[-1], (Hy-validation_losses[-1])/Hy ))\n",
        "      w, n = update_wL2(w, n, x, p, target) #CODE UPDATED HERE \n",
        "      n_updates += 1\n",
        "\n",
        "x = [10000*i for i in range(len(training_losses))]\n",
        "plt.plot(x, training_losses, label='Train')\n",
        "plt.plot(x, validation_losses, label='Validation')\n",
        "plt.xlabel('Number of updates')\n",
        "plt.ylabel('Log Loss')\n",
        "plt.legend( ('Train', 'Validation') )\n",
        "plt.show() \n",
        "\n",
        "wL2 = w #keep it for later use\n",
        "\n",
        "#Depending on the learning rate and the coefficent in front of regularization this is possible \n",
        "#to have some overfitting advocating for early stopping. Remark that it does not hurt as hard the validation performance \n",
        "#Gains remains quite unclear, possibly due to the small dataset size   "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-931bed89f856>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    w[i] -= ((p - y + ) * 2 + lbd * sign(w[i])) * alpha / (sqrt(n[i]) + 1.)\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyG9M30Qnh-S"
      },
      "source": [
        "#Restart the main loop. This does not look as an optimal strategy. \n",
        "D = 2 ** 24\n",
        "w = [0.] * D  \n",
        "n = [0.] * D\n",
        "loss = 0.\n",
        "n_epochs = 5\n",
        "n_updates = 0\n",
        "training_losses = [] \n",
        "validation_losses = [] \n",
        "alpha = .01 \n",
        "\n",
        "for e in range(n_epochs):\n",
        "  training_loss = 0\n",
        "  for t, (row, y)  in enumerate(zip(DictReader(open(X_train)), DictReader(open(y_train)))):\n",
        "      x = get_x(row, D)\n",
        "      p = get_p(x, w)\n",
        "      target = float(y['click'])\n",
        "      training_loss += logloss(p, target)\n",
        "      if n_updates% 10000 == 0 and n_updates>1:\n",
        "          training_losses.append( training_loss/t )\n",
        "          validation_losses.append( compute_validation_loss(w, D) )\n",
        "          print('%s\\tupdates: %d\\tcurrent logloss on train: %f\\tcurrent logloss on validation: %f \\tNCE in validation %f' % (\n",
        "              datetime.now(), n_updates, training_losses[-1], validation_losses[-1], (Hy-validation_losses[-1])/Hy ))\n",
        "      w, n = update_wL1(w, n, x, p, target) #CODE UPDATED HERE \n",
        "      n_updates += 1\n",
        "\n",
        "x = [10000*i for i in range(len(training_losses))]\n",
        "plt.plot(x, training_losses, label='Train')\n",
        "plt.plot(x, validation_losses, label='Validation')\n",
        "plt.xlabel('Number of updates')\n",
        "plt.ylabel('Log Loss')\n",
        "plt.legend( ('Train', 'Validation') )\n",
        "plt.show() \n",
        "\n",
        "wL1 = w #keep it for later use\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08lVHf8pltUh"
      },
      "source": [
        "6. Plot an histogram of the values of the parameters without any regularization, with a L1 regularization and with a L2 one. If there is too many values in w you can subsample it. Can you explain the plots?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDFCojoPgnWq"
      },
      "source": [
        "bins = np.linspace(-0.1, 0.1, 100)\n",
        "plt.hist(wL1[:5000], bins, alpha=0.5, label='L1 weights')\n",
        "plt.hist(wL2[:5000], bins, alpha=0.5, label='L2 weights')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('weight distribution')\n",
        "plt.show()\n",
        "\n",
        "#Indeed there is many zeros in the array (even more for L1 which is expected as it induces sparcity). \n",
        "#To have a resonnable plot we can remove them by\n",
        "wL1_np = np.array(wL1)\n",
        "wL2_np = np.array(wL2)\n",
        "wL1_np[wL1_np==0]=np.nan\n",
        "wL2_np[wL2_np==0]=np.nan\n",
        "plt.hist(wL1_np[:20000], bins, alpha=0.5, label='L1 weights')\n",
        "plt.hist(wL2_np[:20000], bins, alpha=0.5, label='L2 weights')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('weight distribution omitting 0')\n",
        "plt.show()\n",
        "#We observe a lot of small values for parameters which is typical of a large scale log reg with quad features and make them hard to interpret. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UORYeKF9IARr"
      },
      "source": [
        "8. Optimize the hyperparameters (the $L_1/L_2$ terms and the step size of the descent). Maybe this is time to use an external tool such as [Weight and Biases](https://wandb.ai/site) (the easy free for personal use path). It is possible to achieve the same results with  [Ray-Tune](https://docs.ray.io/en/latest/tune/index.html) combined with [MLflow](https://mlflow.org/) (the open source way).\n",
        "\n",
        "Some documentation on hyperparamter tuning [here](https://docs.wandb.ai/guides/sweeps/python-api)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVrZaSv5eXPJ"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI1trSi2IAPw"
      },
      "source": [
        "import wandb\n",
        "#Note that is exist the possiblity to create such config in a yaml file more info at https://docs.wandb.ai/guides/track/config\n",
        "#This may be used to store the results of experiments (including models) in a common place\n",
        "#Here we are going to create to config automatically thanks to a sweep but this is not the sole usage\n",
        "\n",
        "sweep_config = {\n",
        "  \"name\" : \"TP1-ENSAE-hyperparameter_search\",\n",
        "  \"method\" : \"random\", #There exist several strategies such as Bayes and it possible to add some early stopping (see Hyperband). Care NCE is not a loss (higher is better)\n",
        "  \"parameters\" : {\n",
        "    \"epochs\" : {\n",
        "      \"values\" : [1, 5, 10]\n",
        "    },\n",
        "    \"learning_rate\" :{\n",
        "      \"min\": 0.0001,\n",
        "      \"max\": 0.1\n",
        "    },\n",
        "    \"l1\":{\n",
        "      \"min\": 0.,\n",
        "      \"max\": 0.1\n",
        "    },\n",
        "    \"l2\":{\n",
        "      \"min\": 0.,\n",
        "      \"max\": 0.1\n",
        "    },\n",
        "  }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XpgeNKc2W-Z"
      },
      "source": [
        "def update_w_elastic(w, n, x, p, y, l1, l2, alpha):\n",
        "    for i in x:\n",
        "        w[i] -= (p - y + 2 * l2 * w[i] + l1*sign(w[i]) ) * alpha / (sqrt(n[i]) + 1.) \n",
        "        n[i] += 1.\n",
        "    return w, n\n",
        "\n",
        "def train():\n",
        "  with wandb.init(reinit=True) as run:\n",
        "    config = wandb.config\n",
        "    D = 2 ** 24\n",
        "    w = [0.] * D  \n",
        "    n = [0.] * D\n",
        "    loss = 0.\n",
        "    n_updates = 0\n",
        "\n",
        "    for e in range(config['epochs']):\n",
        "      for t, (row, y)  in enumerate(zip(DictReader(open(X_train)), DictReader(open(y_train)))):\n",
        "          x = get_x(row, D)\n",
        "          p = get_p(x, w)\n",
        "          target = float(y['click'])\n",
        "          if n_updates% 10000 == 0 and n_updates>1:\n",
        "              wandb.log({\"loss\": compute_validation_loss(w, D),\n",
        "                         \"epoch\": e,\n",
        "                         \"n_updates\":n_updates})\n",
        "          w, n = update_w_elastic(w, n, x, p, target, config['l1'], config['l2'], config['learning_rate'])  \n",
        "          n_updates += 1\n",
        "\n",
        "count = 10 #Indeed the more the better and longer\n",
        "#Using the same sweep_id it is also possible to distribute the training\n",
        "wandb.agent(sweep_id, function=train, count=count) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLdQ-f6RIBXR"
      },
      "source": [
        "9. Pick one or more kind of gradient descent in [this page ](https://ruder.io/optimizing-gradient-descent/index.html) and implement it (AdaGrad / Adam / [Nesterov's acceleration](https://jlmelville.github.io/mize/nesterov.html) / [SAGA](https://arxiv.org/pdf/1407.0202.pdf) are popular choices). Remark than even an averaged gradient descent will double the memory consuption and significantly increase the execution times in Python. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPnHYU3Vgf2m"
      },
      "source": [
        "#Let's go for Adam\n",
        "#Unchecked code, just the minimal elements to insert at the right place in the code\n",
        "\n",
        "#Adam hyperparameters\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epsilon = 1e-8\n",
        "alpha = 1e-2\n",
        "\n",
        "def compute_gradient(p, y):\n",
        "    #Easy to add L1/L2 penalizations if wanted\n",
        "    return (p - y)\n",
        "\n",
        "def update_w_adam(w, m, v, x, p, y):\n",
        "    g = compute_gradient(p, y)\n",
        "    for i in x:\n",
        "        m[i] = beta1*m[i] + (1-beta1) * g\n",
        "        v[i] = beta2*v[i] + (1-beta2) * g**2\n",
        "        w[i] -= alpha/(sqrt(v[i])+epsilon) * m[i] \n",
        "    return w, m, v\n",
        "\n",
        "#And in the main code include\n",
        "m = [0.] * D\n",
        "v = [0.] * D\n",
        "\n",
        "w, m, v = update_w_adam(w, m, v, x, p, target) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyRCV7CMH__Q"
      },
      "source": [
        "9. A possibility for relatively small datasets is to create a pipeline using sklearn a minimal implementation. Compare the results with the hand coded solution.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "zqMBwXDlH_xr",
        "outputId": "4c6f320f-1a9b-4f36-9ce1-daa3306ad0e6"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "X_train = np.loadtxt('X_train.csv', skiprows=1, delimiter=',')\n",
        "X_train = [[('%s' % _, 1) for _ in x] for x in X_train]\n",
        "y_train = np.loadtxt('y_train.csv', skiprows=1, delimiter=',').astype(np.int8)\n",
        "y_train = y_train[:, 0] #select click data only\n",
        "\n",
        "X_valid = np.loadtxt('X_valid.csv', skiprows=1, delimiter=',')\n",
        "X_valid = [[('%s' % _, 1) for _ in x] for x in X_valid]\n",
        "y_valid = np.loadtxt('y_valid.csv', skiprows=1, delimiter=',').astype(np.int8)\n",
        "y_valid = y_valid[:, 0] \n",
        "\n",
        "\n",
        "# learn model\n",
        "model = make_pipeline(FeatureHasher(n_features=2*6, input_type='pair'), \n",
        "                        PolynomialFeatures(2, interaction_only=False, include_bias=True),\n",
        "                        #LogisticRegressionCV(n_jobs=4, Cs=10, solver='liblinear') #If you want to cross validate the regularization on train\n",
        "                        LogisticRegression(solver='liblinear') #If you want a simple logistic regression\n",
        "      )\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#compute score in validation\n",
        "y_hat_valid = model.predict_proba(X_valid)[:,1] #Output of logistic has as many columns than possible classes\n",
        "print( f'Log loss on validation: {log_loss(y_valid, y_hat_valid)}') #CARE: the option \"normalize\" for log loss is not the NCE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-85f43fb53b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                         \u001b[0;31m#LogisticRegression(solver='liblinear') #If you want a simple logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       )\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#compute score in validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1511\u001b[0m                                           self.include_bias)\n\u001b[1;32m   1512\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_input_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_output_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1511\u001b[0m                                           self.include_bias)\n\u001b[1;32m   1512\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_input_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_output_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S43bMuQDUPZk"
      },
      "source": [
        "Not that bad for some basic Python and SGD *versus* C++ code and a second order method. \n",
        "\n",
        "(optional since it can require long exe time) Can you recover the performance we had by optimizing hyperparameters? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUp-ZnxGH_nc"
      },
      "source": [
        "#Part II. Graded.\n",
        "\n",
        "**Due date december 1st**\n",
        "You can also send your answers to the optional questions \n",
        "\n",
        "1. Based on the minimal code example with validation and $L_2$ prenality, implement your own version of a factorization machine. As we do want to use fields this is the same than a rank k asumption on the matrix of the second order terms which can be expressed as $<v_i, v_j>$ with $v_i \\in \\mathbb{R}^k$ .  Remind that \n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\theta} \\hat{y}(\\mathbf{x})= \\begin{cases}1, & \\text { if } \\theta \\text { is } \\mbox{the bias term } \\omega_0 \\\\ x_{i}, & \\text { if } \\theta \\text { is } w_{i} \\mbox{ (first order term)} \\\\ x_{i} \\sum_{j=1}^{n} v_{j, f} x_{j}-v_{i, f} x_{i}^{2}, & \\text { if } \\theta \\text { is the } i,f \\mbox { second order term } <v_i,v_f>   \\end{cases}\n",
        "$$\n",
        "\n",
        "One possibility to code it is to isolate the second order term within a second vector of parameters while performing the ```get_x``` and change the way the gradient is computed in the ```update_x``` function. Details about factorization machines are in the [original paper](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdXcQ7MsH_HB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iALIM1szWUGa"
      },
      "source": [
        "2. Optimize the hyperparameters of a factorization machine for several dataset sizes and plot the resulting curve -- use the data contained in the test file to explore bigger than 100k training data size. For this question you can use a library such as [libFM](https://libfm.org). Remind that we already used the 10k first lines of the test data to build our validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dquNkmNWT44"
      },
      "source": [
        "import numpy as np\n",
        "X_train = np.loadtxt('X_train.csv', skiprows=1, delimiter=',')\n",
        "X_train = [[('%s' % _, 1) for _ in x] for x in X_train] #Data format expected by the Feature Hasher is a sequence of key,value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2hA1rbTNSw5",
        "outputId": "58000afc-a74a-4a05-c2f5-86e475447aaa"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('479625.0', 1),\n",
              " ('-432181.0', 1),\n",
              " ('480789.0', 1),\n",
              " ('209666.0', 1),\n",
              " ('-326595.0', 1),\n",
              " ('-356697.0', 1),\n",
              " ('167271.0', 1),\n",
              " ('-201336.0', 1),\n",
              " ('-291026.0', 1),\n",
              " ('39558.0', 1),\n",
              " ('-401903.0', 1),\n",
              " ('441172.0', 1),\n",
              " ('119767.0', 1),\n",
              " ('-102727.0', 1),\n",
              " ('-76683.0', 1),\n",
              " ('32790.0', 1),\n",
              " ('326477.0', 1),\n",
              " ('341383.0', 1),\n",
              " ('346045.0', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-VdE1BmNV3s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}